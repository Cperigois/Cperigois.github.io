var store = [,{
    "title": "Beyond the Lab",
    "excerpt":"Current projects                                                                                                                       Search for anomalies in Time Series                                                    Developpement of an autoencoder to detect anomalies such as drift in sinusoïdal signals.                                                   Learn more                                                                                                                                                    Palm oil farms searches                                                    Deep learning algorithm to detect palm oil farms from satellite images. Under developement                                                   Learn more                                                                                                                                                    Lectures for python beginners                                                    From python basics to a well built architecture                                                   Learn more                                        ","url": "http://localhost:4000/Cperigois.github.io/BeyondTheLab/"
  },{
    "title": "Recherche d'anomalie dans une série temporelle",
    "excerpt":"                                       Cette étude s’incrit dans une curiosité de ma part sur l’implémentation d’auteencodeur pour detecteur des anomalies electoniques en analyse du signal. Ce type d’étue est déja largement documenté sur paper with codes en voici quelques bonnes références :   . Pour moi c’est l’occasion de me formé a un nouveau domaine du deep learning les autoencodeurs                 appliqués aux séries temporelles que je connais déja bien.           L’objectif de cette étude est de tater les limite de l’utilisation d’autoencodeurs dans la recherche d’anomalies dans les signaux temporels. La question etant très vaste je me suis limité ici aux signaux sinusoidaux affecté par du bruit gaussien et un bruit de dérive typique de l’usure de composants electroniques. Pour ce billet je vous propose une petit résumé du fonctionnement des autoencodeurs, accompagné des parametres que j’ai choisit pour cette étude. Dans unes second partie de detaillerai la stratégie d’entrainement “étape par étape” qui permet de palier au probleme de périodicité de la fonction sinus. Enfin dans la derniere partie le model sera mis à l’épreuve avec une évaluation et des tests sur des données bruitées par un ratio de dérive.   Model IA   La base des autoencodeurs   “Ecrit une introduction basique aux autoencodeurs”   Mon model   “Voila mon model : class CNNEncoder(nn.Module):     def init(self, encoded_size):         super(CNNEncoder, self).init()         self.encoder = nn.Sequential(             nn.Conv1d(1, 16, kernel_size=5, stride=2, padding=2),  # [B, 16, 250]             nn.ReLU(),             nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),  # [B, 32, 125]             nn.ReLU(),             nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),  # [B, 64, ~63]             nn.ReLU(),             nn.Flatten(),             nn.Linear(64 * 63, encoded_size)         )   def forward(self, x):     return self.encoder(x)   class CNNDecoder(nn.Module):     def init(self, encoded_size):         super(CNNDecoder, self).init()         self.decoder_input = nn.Linear(encoded_size, 64 * 63)         self.decoder = nn.Sequential(             nn.Unflatten(1, (64, 63)),             nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1),  # [B, 32, ~125]             nn.ReLU(),             nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),  # [B, 16, ~250]             nn.ReLU(),             nn.ConvTranspose1d(16, 1, kernel_size=5, stride=2, padding=2, output_padding=1),   # [B, 1, ~500]         )   def forward(self, x):     x = self.decoder_input(x)     x = self.decoder(x)     return x   class CNNAutoencoder(nn.Module):     def init(self, encoded_size):         super(CNNAutoencoder, self).init()         self.encoder = CNNEncoder(encoded_size)         self.decoder = CNNDecoder(encoded_size)         self.encoded_size = encoded_size   def forward(self, x):     latent = self.encoder(x)     reconstructed = self.decoder(latent)     return reconstructed ecrit un court paragraphe pour le décrire \"   Entrainement du model   Les données d’entrainement   L’entrainement est réalisé sur des sets de 12000 signaux de 500s, envoyés par patch de 32 au GPU.   La stratégie étape par étape   “L’entrainement du modele é été réalisé étape par étape, voici les différentes étapes et leurs caractéristique, il faudrait les repertorier dans un tableau. Etape 1 : La sunisoïde pure, sur 10 epochs, amplitude 4, phase 0, fréquence 0.05Hz, bruit Aucun Etape 2 : Légere variation de l’amplitude, sur 30 epochs, amplitude Uniform(3.5,4.5), phase 0, fréquence 0.05Hz, bruit Aucun Etape 3 : Et légère variation de la phase, sur 30 epochs, amplitude Uniform(3.5,4.5), phase Uniform(0,0.5), fréquence 0.05Hz, bruit Aucun Etape 4 : Et légère variation de la fréquence, sur 100 epochs, amplitude Uniform(3.5,4.5), phase Uniform(0,0.5), fréquence Uniform(0.025,0.075)Hz, bruit Aucun Etape 5 : Et ajout d’un faible bruit gaussien, sur 100 epochs, amplitude Uniform(3.5,4.5), phase Uniform(0,0.5), fréquence Uniform(0.025,0.075)Hz, bruit Loi normale(mu = 0, std = 0.05) Etape 6 : Augmantation de la variation des parametre de la sinusoïde, sur 250 epochs, amplitude Uniform(2,6), phase Uniform(0,2pi), fréquence Uniform(0.025,0.225)Hz, bruit Loi normale(mu = 0, std = 0.05) Etape 7 : Augmantation du bruit gaussien, sur 400 epochs, amplitude Uniform(2,6), phase Uniform(0,2pi), fréquence Uniform(0.025,0.225)Hz, bruit Loi normale(mu = 0, std = 0.25) Chaque étape de l’entrainement démarre avec un learning rate assez élevé de 1.e-4. Avec un scheduler, ReduceLROnPlateau, qui a chaque epoche verifie que l’entrainement d’ai pas atteint un seuil qui l’empeche de progresser, et si c’est le cas diminue le mearning rate. “   Le bruit résiduel   LEs signaux contenant du bruit gaussien il est impossible pour le modele de prédire parfaitement le signal d’entrée. Il y a donc dans cette étude un bruit résiduel. Si l’on reprend la formule de notre fonction de perte cf eqation précédente, on peut y decomposer le signal target comme la somme d’un signal informatif et d’un bruit aléatoire. En supposant que notre modele est parfait on a alors output = signal informatif. Il reste dons une erreur résiduelle :  mse = mean((output - (informativ_signal + random_noise)) ** 2) residual error = mean(random_noise ** 2) Dans le cas de notre étude le bruit résiduel sur la fonction de perte est de 0.0025 pour le bruit gaussien des étapes 4-6, puis de 0.0625 pour l’étape finale d’entrainement.   Ecrit une place pour que j’insert ici un graphique “sinusoïd_recognition.png” réprésentant l’évolution de la loss de validation pendant l’entrainement avec une légende que je pourrait modifier.   Evaluation et test sur des données bruitées   Evaluation du model   Le model est évalué sur 10000 nouveaux signaux générés comme a l’étape 7 de l’entrainement. On retrouve une valeur médiane de la perte de 0.078+0.053-0.012 ce qui est suffisament proche de l’erreur résiduelle pour notre cas d’études.   Les données bruitées  On mat notre model d’autoencodeur a l’épreuve d’un bruit de dérive “petite definition d’un bruit de dérive et formula mathématique ici”   Voici quelques exemple de signaux obtenus apres ajout du bruit de dérive pour quatra valeur différente “Ici j’insere une image avec 4 graphe représentant le signal avec sa composante dérive et sa reconstitution par le model”   Performance de détection d’anomalie   Comme on l’attendait, le model n’arrive pas a reproduire le signal d’entrée si il y a un bruit supplémentaire auquel il n’est pas entrainé. Sur le graphique suivant j’ai quantifié le score de perte de 1000 signaux pour 8 valeurs différentes de ratios de dérive. J’ai choisit de placer trois zones de confiance, lors du calcul de la perte.  OK : lorsque la perte est inférieur au 95 eme percentille de notre sample d’évaluation, le signal est avec confiance considéré comme normal. WARNING : lorsque la perte est comprise entre le 95eme percentille de du sample d’évaluation mais inférieure a la valaur maximale obtenue dans l’évaluation, le signal est considéré comme a risque, et la tendance doit etre surveillée. ANOMALY : au dela de la valeur maximale de perte obtenue dans le sample d’évaluation le signal est considéré comme anormale et des investigations plus poussées sont necessaires   Pour avoir une valeur comparable des résultats avec d’autres problemes j’ai évalué le Noise ratio defini comme Noise ratio = total_drift/residual error avec total_drift la dérive totale du signal sur sa durée. On constate que notre model detecte des anomalies de derive a partir de 1.6*bruit résiduel, ce qui est très performant. Ces performance pourraient etre encore augmenté en prenant des signaux plus long pour accentuer la dérive totale.   Conclusion   Les autoencodeurs sont efficace pour detecter des anomalies dans les signaux sinusoidaux bruités avec un ratio de dérive léger. Lorque ce ratio se rapprohe du bruit gaussien avec un Noise ratio &lt; 1.6, le bruit devient trop faible pour etre discerné du bruit blanc sur  la durée évaluée ici 500 secondes.   Ouverture : faire des test sur des signaux plus long permet de mettre en évidente des ratio de dérive plus faible.  J’ai affirmé dans mon étude que le probleme rencontré a l’entrainement direct avec les données complexee provenait e la caractéristique sinusoidale du signale, qui automatiquement donne un signal moyen null. Cette affiramtion mérite une investigation plus large (peut etre une future étude),il sera possible d’ajouter un offser, de travailler avec des fonctions de heavyside, ou encore des triangle. Ce cas reste un cas d’école, prototypé avec des données construitent, il faudrait mettre cette strategie a l’épreuve sur des données réelles.   ","url": "http://localhost:4000/Cperigois.github.io/IAlab/"
  },{
    "title": "Recherche d'anomalie dans une série temporelle",
    "excerpt":"                                       Cette étude s’inscrit dans une curiosité personnelle concernant l’implémentation d’autoencodeurs pour la détection d’anomalies électroniques en analyse du signal. Ce type d’approche est déjà largement documenté sur Papers With Code ; voici quelques références pertinentes. Pour moi, c’est également l’occasion de me former à un nouveau domaine du deep learning : les autoencodeurs appliqués aux séries temporelles, un sujet que je maîtrise déjà bien du point de vue signal.           L’objectif de cette étude est de tester les limites de l’utilisation d’autoencodeurs dans la recherche d’anomalies dans les signaux temporels. Le sujet étant très vaste, je me suis limité ici à l’étude de signaux sinusoïdaux affectés par un bruit gaussien et un bruit de dérive, ce dernier étant typique de l’usure de composants électroniques.   Ce billet se compose de trois parties :     Un résumé du fonctionnement des autoencodeurs, avec les paramètres choisis pour cette étude.   La stratégie d’entraînement « étape par étape » pour contourner les difficultés liées à la périodicité des signaux.   Une évaluation du modèle sur des données bruitées contenant un ratio de dérive contrôlé.     Modèle AI   La base des autoencodeurs   Un autoencodeur est un type de réseau de neurones dont l’objectif est de reconstruire l’entrée après l’avoir compressée dans un espace latent de plus faible dimension. Il se compose de deux parties :      L’encodeur : réduit la dimensionnalité de l’entrée pour extraire ses caractéristiques essentielles.   Le décodeur : reconstruit l’entrée à partir de la représentation compressée.   En apprentissage non supervisé, un autoencodeur peut ainsi apprendre les motifs dominants des données. Toute erreur de reconstruction importante indique alors une anomalie (c’est-à-dire une donnée ne correspondant pas au motif appris).   La fonction de perte classique est l’erreur quadratique moyenne :   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x}_i)^2\\]  où \\(x_i\\) est la donnée originale et \\(\\hat{x}_i\\) sa reconstruction.     Mon modèle   Le modèle utilisé dans cette étude est un autoencodeur convolutionnel 1D conçu pour traiter des signaux temporels de longueur fixe (500 échantillons). Il se compose de deux blocs principaux : un encodeur et un décodeur, construits à partir de couches convolutives, adaptées aux données structurées dans le temps.   Encodeur   L’encodeur transforme le signal d’entrée en une représentation latente de dimension réduite (embedding). Il est constitué de :   Trois couches de convolution 1D successives avec :           Un nombre croissant de canaux (16, puis 32, puis 64),            Un noyau de convolution de taille 5,            Un stride de 2 pour réduire progressivement la résolution temporelle,            Une activation ReLU après chaque convolution.       À la fin de ces couches, la sortie est applatie (flatten) pour former un vecteur linéaire, qui est ensuite projeté via une couche linéaire (fully connected) vers l’espace latent de taille définie (encoded_size).   Décodeur   Le décodeur reconstruit le signal original à partir de la représentation latente. Il réalise l’opération inverse de l’encodeur, avec :   Une première couche linéaire pour transformer le vecteur latent en un tenseur compatible avec la structure attendue par les couches de déconvolution.   Trois couches de convolution transposée (aussi appelées “déconvolutions”) :   Elles augmentent progressivement la taille du signal dans le temps,   Réduisent le nombre de canaux de 64 à 32, puis 16, puis 1 (la forme du signal d’origine),   Chaque étape est suivie d’une activation ReLU, sauf la dernière.   Ce design permet au modèle d’apprendre à extraire les motifs caractéristiques du signal d’entrée, puis à les reconstruire aussi fidèlement que possible. La compression dans l’espace latent force l’autoencodeur à filtrer le bruit et à ne conserver que les informations essentielles.   Ce modèle encode des signaux d’entrée de taille 500 vers un espace latent de dimension fixée à 30% de la taille d’entrée dans notre cas, avant de les reconstituer via des couches convolutionnelles transposées. L’architecture est volontairement compacte pour faciliter l’apprentissage progressif de motifs simples.     Entraînement du modèle   Les données d’entraînement   L’entraînement est réalisé sur des lots de 12000 signaux de 500 secondes, envoyés par batchs de 32 au GPU. Chaque signal est une sinusoïde bruitée, générée aléatoirement selon les paramètres de l’étape d’apprentissage.     Stratégie d’apprentissage par étapes   L’entraînement suit une progression par complexification croissante des signaux. Voici un résumé en tableau :                  Étape       Époques       Amplitude       Phase       Fréquence (Hz)       Bruit                       1       10       4       0       0.05       Aucun                 2       30       \\(\\mathcal{U}(3.5, 4.5)\\)       0       0.05       Aucun                 3       30       \\(\\mathcal{U}(3.5, 4.5)\\)       \\(\\mathcal{U}(0, 0.5)\\)       0.05       Aucun                 4       100       \\(\\mathcal{U}(3.5, 4.5)\\)       \\(\\mathcal{U}(0, 0.5)\\)       \\(\\mathcal{U}(0.025, 0.075)\\)       Aucun                 5       100       \\(\\mathcal{U}(3.5, 4.5)\\)       \\(\\mathcal{U}(0, 0.5)\\)       \\(\\mathcal{U}(0.025, 0.075)\\)       \\(\\mathcal{N}(0, 0.05)\\)                 6       250       \\(\\mathcal{U}(2, 6)\\)       \\(\\mathcal{U}(0, 2π)\\)       \\(\\mathcal{U}(0.025, 0.225)\\)       \\(\\mathcal{N}(0, 0.05)\\)                 7       400       \\(\\mathcal{U}(2, 6)\\)       \\(\\mathcal{U}(0, 2π)\\)       \\(\\mathcal{U}(0.025, 0.225)\\)       \\(\\mathcal{N}(0, 0.25)\\)           Un scheduler de type ReduceLROnPlateau est utilisé pour ajuster dynamiquement le learning rate à partir d’une valeur initiale de ( 10^{-4} ), en cas de stagnation des performances.     Le bruit résiduel   Le modèle ne peut reconstruire parfaitement le signal d’entrée, à cause de la composante aléatoire du bruit. La perte finale atteint donc une valeur non nulle, appelée bruit résiduel.   Considérons :   \\[\\text{signal}_{\\text{target}} = \\text{signal}_{\\text{informatif}} + \\text{bruit}_{\\text{aléatoire}}\\]  et supposons une reconstruction parfaite du signal informatif. Alors, l’erreur moyenne devient :   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n [(\\text{output} - \\text{signal}_{\\text{target}})^2] = \\frac{1}{n} \\sum_{i=1}^n [\\text{bruit}_{\\text{aléatoire}}^2]\\]  Dans notre étude :     Étapes 4–6 : bruit résiduel ≈ 0.0025   Étape 7 : bruit résiduel ≈ 0.0625      Évolution de la perte de validation durant l’entraînement (légende à adapter).     Évaluation et tests sur signaux bruités   Évaluation du modèle   Le modèle est évalué sur 10 000 nouveaux signaux générés avec les paramètres de l’étape 7.  La perte médiane observée est :   \\[\\text{MSE}_{\\text{test}} = 0.078^{+0.053}_{-0.012}\\]  Ce qui reste compatible avec le bruit résiduel, indiquant un bon pouvoir de généralisation.     Les données bruitées   Un bruit de dérive a été ajouté aux signaux :      Une dérive est une modification lente et continue du signal dans le temps. Elle peut être modélisée par une fonction affine :    \\[\\text{drift}(t) = \\alpha \\cdot t\\]  où ( \\alpha ) est le coefficient de dérive.      Exemples de signaux avec dérive pour quatre valeurs de ( \\alpha ), comparés à leur reconstruction.     Détection d’anomalie   Le modèle échoue à reconstruire correctement les signaux contenant une dérive non apprise, comme attendu. Pour chaque ratio de dérive, la perte est comparée aux percentiles de l’évaluation :      OK : MSE &lt; 95ᵉ percentile   WARNING : 95ᵉ percentile &lt; MSE &lt; max   ANOMALY : MSE &gt; max (évaluation)   Pour quantifier le seuil de détection, on définit le noise ratio :   \\[\\text{Noise ratio} = \\frac{\\text{total\\_drift}}{\\text{erreur résiduelle}}\\]  Le modèle détecte des anomalies dès que le noise ratio atteint 1.6, ce qui indique une très bonne sensibilité. Cette performance pourrait être améliorée en augmentant la durée des signaux.      Exemples de signaux avec dérive pour quatre valeurs de ( \\alpha ), comparés à leur reconstruction.     Conclusion   Les autoencodeurs se montrent efficaces pour détecter des anomalies dans des signaux sinusoïdaux contaminés par du bruit gaussien et de la dérive. La méthode est particulièrement sensible aux déviations progressives, tant que leur amplitude dépasse le bruit résiduel.   Perspectives :     Travailler sur des signaux plus longs pour détecter des dérives plus faibles.   Explorer des alternatives aux sinusoïdes : signaux triangulaires, fonctions de Heaviside, etc.   Vérifier l’hypothèse selon laquelle les difficultés d’entraînement proviennent de la moyenne nulle des sinusoïdes — en ajoutant par exemple un offset.   Ce travail constitue un prototype sur données simulées. Il serait pertinent de valider cette approche sur des données réelles pour en mesurer le potentiel industriel.    ```  ","url": "http://localhost:4000/Cperigois.github.io/BeyondTheLab/IALab/Search_anomaly"
  },{
    "title": "Recherche d'anomalie dans une série temporelle",
    "excerpt":"                                       Cette étude s’inscrit dans une curiosité personnelle concernant l’implémentation d’autoencodeurs pour la détection d’anomalies électroniques en analyse du signal. Ce type d’approche est déjà largement documenté sur Papers With Code ; voici quelques références pertinentes. Pour moi, c’est également l’occasion de me former à un nouveau domaine du deep learning : les autoencodeurs appliqués aux séries temporelles, un sujet que je maîtrise déjà bien du point de vue signal.           L’objectif de cette étude est de tester les limites de l’utilisation d’autoencodeurs dans la recherche d’anomalies dans les signaux temporels. Le sujet étant très vaste, je me suis limité ici à l’étude de signaux sinusoïdaux affectés par un bruit gaussien et un bruit de dérive, ce dernier étant typique de l’usure de composants électroniques.   Ce billet se compose de trois parties :     Un résumé du fonctionnement des autoencodeurs, avec les paramètres choisis pour cette étude.   La stratégie d’entraînement « étape par étape » pour contourner les difficultés liées à la périodicité des signaux.   Une évaluation du modèle sur des données bruitées contenant un ratio de dérive contrôlé.     Modèle IA   La base des autoencodeurs   Un autoencodeur est un type de réseau de neurones dont l’objectif est de reconstruire l’entrée après l’avoir compressée dans un espace latent de plus faible dimension. Il se compose de deux parties :      L’encodeur : réduit la dimensionnalité de l’entrée pour extraire ses caractéristiques essentielles.   Le décodeur : reconstruit l’entrée à partir de la représentation compressée.   En apprentissage non supervisé, un autoencodeur peut ainsi apprendre les motifs dominants des données. Toute erreur de reconstruction importante indique alors une anomalie (c’est-à-dire une donnée ne correspondant pas au motif appris).   La fonction de perte classique est l’erreur quadratique moyenne :   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x}_i)^2\\]  où ( x_i ) est la donnée originale et ( \\hat{x}_i ) sa reconstruction.     Mon modèle   Le modèle utilisé dans cette étude est un autoencodeur convolutionnel 1D conçu pour traiter des signaux temporels de longueur fixe (500 échantillons). Il se compose de deux blocs principaux : un encodeur et un décodeur, construits à partir de couches convolutives, adaptées aux données structurées dans le temps.   Encodeur   L’encodeur transforme le signal d’entrée en une représentation latente de dimension réduite (embedding). Il est constitué de :   Trois couches de convolution 1D successives avec :           Un nombre croissant de canaux (16, puis 32, puis 64),            Un noyau de convolution de taille 5,            Un stride de 2 pour réduire progressivement la résolution temporelle,            Une activation ReLU après chaque convolution.       À la fin de ces couches, la sortie est applatie (flatten) pour former un vecteur linéaire, qui est ensuite projeté via une couche linéaire (fully connected) vers l’espace latent de taille définie (encoded_size).   Décodeur   Le décodeur reconstruit le signal original à partir de la représentation latente. Il réalise l’opération inverse de l’encodeur, avec :   Une première couche linéaire pour transformer le vecteur latent en un tenseur compatible avec la structure attendue par les couches de déconvolution.   Trois couches de convolution transposée (aussi appelées “déconvolutions”) :   Elles augmentent progressivement la taille du signal dans le temps,   Réduisent le nombre de canaux de 64 à 32, puis 16, puis 1 (la forme du signal d’origine),   Chaque étape est suivie d’une activation ReLU, sauf la dernière.   Ce design permet au modèle d’apprendre à extraire les motifs caractéristiques du signal d’entrée, puis à les reconstruire aussi fidèlement que possible. La compression dans l’espace latent force l’autoencodeur à filtrer le bruit et à ne conserver que les informations essentielles.   Ce modèle encode des signaux d’entrée de taille 500 vers un espace latent de dimension fixée à 30% de la taille d’entrée dans notre cas, avant de les reconstituer via des couches convolutionnelles transposées. L’architecture est volontairement compacte pour faciliter l’apprentissage progressif de motifs simples.     Entraînement du modèle   Les données d’entraînement   L’entraînement est réalisé sur des lots de 12000 signaux de 500 secondes, envoyés par batchs de 32 au GPU. Chaque signal est une sinusoïde bruitée, générée aléatoirement selon les paramètres de l’étape d’apprentissage.     Stratégie d’apprentissage par étapes   L’entraînement suit une progression par complexification croissante des signaux. Voici un résumé en tableau :                  Étape       Époques       Amplitude       Phase       Fréquence       Bruit                       1       10       4       0       0.05 Hz       Aucun                 2       30       Uniform(3.5, 4.5)       0       0.05 Hz       Aucun                 3       30       Uniform(3.5, 4.5)       Uniform(0, 0.5)       0.05 Hz       Aucun                 4       100       Uniform(3.5, 4.5)       Uniform(0, 0.5)       Uniform(0.025, 0.075) Hz       Aucun                 5       100       Uniform(3.5, 4.5)       Uniform(0, 0.5)       Uniform(0.025, 0.075) Hz       Bruit gaussien ( \\mathcal{N}(0, 0.05) )                 6       250       Uniform(2, 6)       Uniform(0, 2π)       Uniform(0.025, 0.225) Hz       Bruit gaussien ( \\mathcal{N}(0, 0.05) )                 7       400       Uniform(2, 6)       Uniform(0, 2π)       Uniform(0.025, 0.225) Hz       Bruit gaussien ( \\mathcal{N}(0, 0.25) )           Un scheduler de type ReduceLROnPlateau est utilisé pour ajuster dynamiquement le learning rate à partir d’une valeur initiale de ( 10^{-4} ), en cas de stagnation des performances.     Le bruit résiduel   Le modèle ne peut reconstruire parfaitement le signal d’entrée, à cause de la composante aléatoire du bruit. La perte finale atteint donc une valeur non nulle, appelée bruit résiduel.   Considérons :   \\[\\text{signal}_{\\text{target}} = \\text{signal}_{\\text{informatif}} + \\text{bruit}_{\\text{aléatoire}}\\]  et supposons une reconstruction parfaite du signal informatif. Alors, l’erreur moyenne devient :   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n [(\\text{output} - \\text{signal}_{\\text{target}})^2] = \\frac{1}{n} \\sum_{i=1}^n [\\text{bruit}_{\\text{aléatoire}}^2]\\]  Dans notre étude :     Étapes 4–6 : bruit résiduel ≈ 0.0025   Étape 7 : bruit résiduel ≈ 0.0625      Évolution de la perte de validation durant l’entraînement (légende à adapter).     Évaluation et tests sur signaux bruités   Évaluation du modèle   Le modèle est évalué sur 10 000 nouveaux signaux générés avec les paramètres de l’étape 7.  La perte médiane observée est :   \\[\\text{MSE}_{\\text{test}} = 0.078^{+0.053}_{-0.012}\\]  Ce qui reste compatible avec le bruit résiduel, indiquant un bon pouvoir de généralisation.     Les données bruitées   Un bruit de dérive a été ajouté aux signaux :      Une dérive est une modification lente et continue du signal dans le temps. Elle peut être modélisée par une fonction affine :    \\[\\text{drift}(t) = \\alpha \\cdot t\\]  où ( \\alpha ) est le coefficient de dérive.      Exemples de signaux avec dérive pour quatre valeurs de ( \\alpha ), comparés à leur reconstruction.     Détection d’anomalie   Le modèle échoue à reconstruire correctement les signaux contenant une dérive non apprise, comme attendu. Pour chaque ratio de dérive, la perte est comparée aux percentiles de l’évaluation :      OK : MSE &lt; 95ᵉ percentile   WARNING : 95ᵉ percentile &lt; MSE &lt; max   ANOMALY : MSE &gt; max (évaluation)   Pour quantifier le seuil de détection, on définit le noise ratio :   \\[\\text{Noise ratio} = \\frac{\\text{total\\_drift}}{\\text{erreur résiduelle}}\\]  Le modèle détecte des anomalies dès que le noise ratio atteint 1.6, ce qui indique une très bonne sensibilité. Cette performance pourrait être améliorée en augmentant la durée des signaux.      Exemples de signaux avec dérive pour quatre valeurs de ( \\alpha ), comparés à leur reconstruction.     Conclusion   Les autoencodeurs se montrent efficaces pour détecter des anomalies dans des signaux sinusoïdaux contaminés par du bruit gaussien et de la dérive. La méthode est particulièrement sensible aux déviations progressives, tant que leur amplitude dépasse le bruit résiduel.   Perspectives :     Travailler sur des signaux plus longs pour détecter des dérives plus faibles.   Explorer des alternatives aux sinusoïdes : signaux triangulaires, fonctions de Heaviside, etc.   Vérifier l’hypothèse selon laquelle les difficultés d’entraînement proviennent de la moyenne nulle des sinusoïdes — en ajoutant par exemple un offset.   Ce travail constitue un prototype sur données simulées. Il serait pertinent de valider cette approche sur des données réelles pour en mesurer le potentiel industriel.    ```  ","url": "http://localhost:4000/Cperigois.github.io/IAlab/"
  },{
    "title": {"en":"Black holes & gravitational waves","fr":"Trous noirs & ondes gravitationnelles"},
    "excerpt":"Pour introduire mes travaux de recherche je propose de les integrer dans trois présentations vulgarisées plus larges qui permettront d’evaluer le contexte général des projets que j’ai eu le plaisir de mener pendant ma carriere. Vous trouverez donc trois pages de recherche :      Les ondes gravitationnelles et leur détections   Aux origines des trous noirs   ??   Sur cette première page j’introduis les ondes gravitationnelles dns le but de présenter mes travaux de recherches et les projets  auxquels j’ai eu le plaisir de participer.   Une introduction historique  Les ondes gravitationnelles sont des perturbations de l’espace temps qui se propagent à la vitesse de la lumière. Elles  sont une conséquence directe de la theorie de la relativité générales d’Albert Einstein et des ses collaborateurs.ices  soumise en 1915, sous la forme de deux papiers principaux : “On the general theory of relativity”  et “The Field Equations of Gravitation”. La première publication  formalisant les ondes gravitationnelles “Über Gravitationswellen”  quant a elle arrive 3 ans plus tard.                                         Abstract de l’article original sur les ondes gravitationnelles en 1918 par Albert Einstein           En 1974, Rusell Hulse et Joseph Taylor, découvrent le pulsar binaire PSR B1913+16, c’est a dire un système de deux objet  en orbite dont l’un est un pulsar, qui a intervalle régulier émet un pulse de lumière. En 1979 après quatre ans de  surveillance des caratérisiques du systèmes, Hulse et Taylor mettent en évidence que l’évolution de l’orbite du systeme  reproduit parfaitement les prédictions de la théorie de la gravitation pour un système binaire (Lien vers la publication. Cette découverte leur apportera un prix nobel en 1993. La publication la plus récente a ce jour (2016) qui répertorie ces  mesures est en libre acces à ce lien : Denière publication de 2016                                         Graphique représentant la décroissance de la période du pulsar binaire. Les points rouges représentent les prises de données, avec leur incertitudes (quasi invisible car faibles). La courbe représente la prédiction de la théorie de la gravitation           C’est l’année du centième anniversaire de la théorie de la relativité générale, en 2015, que la première mesure directe  d’onde gravitationnelles à eu lieu. Les détecteurs américains LIGO détecter alors la fusion de deux trous noirs de  respectivement 29 et 36 masses solaires (Lien vers l’article)   A partir de 2017,lors de la seconde phase de prise de données (O2), le détecteur européen rejoint le reseau américain, et permet la  traiangulation du signal pour reconstruire la provenance des sources d’ondes gravitaionnelles. Le 17 aout 2017, le nouveau  réseau observe une fusion de deux étoiles à neutrons conjointement avec les ondes electromagnétiques  (Lien vers la publication,  Lien vers l’article de la détection multimessagère. C’est la naissance  le l’astronomie multimessagère c’est à dire la construction d’une communauté pluridisciplinaire, qui permet d’observer les  phénomènes astrophysiques avec différents messagers : les ondes electromagnétiques, les rayons cosmiques, les neutrinos et  les ondes gravitationnelles. C’est également en 2017 que les travaux de la premières détections, GW150914, sont récompensés d’un prix nobel.   Au jour de l’écriture de cette page (2025) le reseaux de détecteurs d’ondes gravitationnel compte 4 instruments, deux  sur le sol americain, un en italie, et un au Japon. La communauté dénombre desormais plus d’une centaine d’observation de  fusions de trous noirs, offrant à la communauté astrophysiques des données précieuses sur l’histoire et l’évolution de l’univers.   Detecter les ondes gravitationnelles      Présentation à la conférence X  Paris, Mars 2022     ","url": "http://localhost:4000/Cperigois.github.io/fr/gravitational-waves/"
  },{
    "title": "Black holes & gravitational waves",
    "excerpt":"Texte justifié :  ...  To introduce my research work, I propose to integrate it into three broader popular presentations that will allow for an  evaluation of the general context of the projects I have had the pleasure of conducting throughout my career. You will find three research pages:      Gravitational waves and their detection   The origins of black holes   ??   On this first page, I introduce gravitational waves with the aim of presenting my research work and the projects I have  had the pleasure of participating in.   An historical introduction   Gravitational waves are disturbances in spacetime that propagate at the speed of light. They are a direct consequence of  Albert Einstein’s general theory of relativity and his collaborators, submitted in 1915, in the form of two main papers: “On the general theory of relativity”  and “The Field Equations of Gravitation”. The first publication  formalizing gravitational waves “Über Gravitationswellen” , came three years later.                                         *Abstract of the original article on gravitational waves in 1918 by Albert Einstein *           In 1974, Russell Hulse and Joseph Taylor discovered the binary pulsar PSR B1913+16, which is a system of two objects in orbit,  one of which is a pulsar that emits a pulse of light at regular intervals. In 1979, after four years of monitoring the  characteristics of the system, Hulse and Taylor demonstrated that the evolution of the system’s orbit perfectly reproduces  the predictions of gravitational theory for a binary system (Link to the publication. This discovery earned them a Nobel Prize in 1993. The most recent publication to date (2016) that catalogs these measurements  is freely accessible at this link:  : Last publication from 2016                                         Graph showing the decay of the binary pulsar’s period. The red points represent data points, with their uncertainties (almost invisible due to being small). The curve represents the prediction of gravitational theory           C’est l’année du centième anniversaire de la théorie de la relativité générale, en 2015, que la première mesure directe  d’onde gravitationnelles à eu lieu. Les détecteurs américains LIGO détecter alors la fusion de deux trous noirs de  respectivement 29 et 36 masses solaires (Lien vers l’article)   A partir de 2017,lors de la seconde phase de prise de données (O2), le détecteur européen rejoint le reseau américain, et permet la  traiangulation du signal pour reconstruire la provenance des sources d’ondes gravitaionnelles. Le 17 aout 2017, le nouveau  réseau observe une fusion de deux étoiles à neutrons conjointement avec les ondes electromagnétiques  (Lien vers la publication,  Lien vers l’article de la détection multimessagère. C’est la naissance  le l’astronomie multimessagère c’est à dire la construction d’une communauté pluridisciplinaire, qui permet d’observer les  phénomènes astrophysiques avec différents messagers : les ondes electromagnétiques, les rayons cosmiques, les neutrinos et  les ondes gravitationnelles. C’est également en 2017 que les travaux de la premières détections, GW150914, sont récompensés d’un prix nobel.   Au jour de l’écriture de cette page (2025) le reseaux de détecteurs d’ondes gravitationnel compte 4 instruments, deux  sur le sol americain, un en italie, et un au Japon. La communauté dénombre desormais plus d’une centaine d’observation de  fusions de trous noirs, offrant à la communauté astrophysiques des données précieuses sur l’histoire et l’évolution de l’univers.   Detecter les ondes gravitationnelles   ","url": "http://localhost:4000/Cperigois.github.io/gravitational-waves/"
  },{
    "title": {"en":"Data science","fr":"Data science"},
    "excerpt":"Projets en cours                                                                                                                       Princess                                                    Outil open source pour prédire les ondes gravitationnelles à partir de modèles astrophysiques.                                                   En savoir plus                                                                                                                                                    Détection de plantations de palmiers                                                    Algorithme de deep learning pour identifier les plantations d’huile de palme sur images satellites. En développement.                                                   En savoir plus                                                                                                                                                    Cours d'initiation à Python                                                    Des bases du langage Python jusqu’à l’architecture logicielle avancée                                                   En savoir plus                                       ","url": "http://localhost:4000/Cperigois.github.io/"
  },{
    "title": "Data science",
    "excerpt":"Current projects                                                                                                                       Princess                                                    Open source tool to predict gravitational waves from astrophysical models.                                                   Learn more                                                                                                                                                    Palm oil farms searches                                                    Deep learning algorithm to detect palm oil farms from satellite images. Under developement                                                   Learn more                                                                                                                                                    Lectures for python beginners                                                    From python basics to a well built architecture                                                   Learn more                                        ","url": "http://localhost:4000/Cperigois.github.io/"
  }]
