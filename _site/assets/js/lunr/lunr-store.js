var store = [,{
    "title": "Beyond the Lab",
    "excerpt":"Current projects                                                                                                                       Search for anomalies in Time Series                                                    Developpement of an autoencoder to detect anomalies such as drift in sinusoïdal signals.                                                   Learn more                                                                                                                                                    Palm oil farms searches                                                    Deep learning algorithm to detect palm oil farms from satellite images. Under developement                                                   Learn more                                                                                                                                                    Lectures for python beginners                                                    From python basics to a well built architecture                                                   Learn more                                        ","url": "http://localhost:4000/Cperigois.github.io/BeyondTheLab/"
  },{
    "title": "Noise modelisation and propagation in gravitational wave detector",
    "excerpt":"To detect gravitational waves with maximum efficiency, interferometers must be maintained at their operating point — ideally on a dark fringe — so that any deviation (from gravitational wave signals) creates detectable interference patterns at the photodiodes. This is achieved through multiple feedback control loops that stabilize the mirror positions and the input laser frequency.   However, these control systems can also serve as channels through which instrumental noise propagates within the detector. A significant part of my PhD work involved modeling these feedback loops to better understand how noise is introduced and transferred throughout the system.   Interferometer Degrees of Freedom   The longitudinal control of the interferometer is based on four main degrees of freedom (DoF), which represent critical optical lengths in the system. These are:           CARM (Common Arm):  \\(CARM = \\frac{L_N + L_W}{2}\\) Controls the average length of the Fabry-Pérot arms using signal from photodiode B2.            DARM (Differential Arm):  \\(DARM = L_N - L_W\\) Key observable for gravitational waves, controlling the end mirror positions.            MICH (Michelson):  \\(MICH = l_N - l_W\\) Measures asymmetry in the short arms, extracted via photodiode B4.            PRCL (Power Recycling Cavity Length):  \\(PRCL = l_0 + \\frac{l_W + l_N}{2}\\) Coupled with MICH, also measured via B4, and used to stabilize the power recycling mirror (PRM).       These DoFs define the longitudinal control architecture.     Interferometer and Control Loop Modeling   The control loop simulation framework, called Noise Budget, includes all major subsystems:      Sensing (green): Converts photodiode signals into error signals for each DoF.   Mirror/Frequency Control (yellow): Converts errors into corrective signals.   Driving (orange): Simulates actual mirror actuation.   Laser (blue): Simulates laser frequency response.   Optickle (violet): Models optical response to all mirror and laser perturbations.   These modules together simulate the closed-loop noise propagation and are used to test new control strategies or predict future interferometer performance.     Study of the MICH Loop   The MICH control loop is responsible for stabilizing short-arm asymmetry.  For frequencies above 50 Hz, the loop is dominated by detection noise (electronic + shot noise).  Assuming this dominance, I was able to derive the injection transfer function ( Y(f) ) for the loop and validate the simulated model against actual interferometer data.  This confirmed the theoretical noise paths and highlighted where detection noise dominates the system.     Study of the SSFS Loop (Laser Frequency Stabilization)   The laser introduces phase noise, which is integrated from frequency fluctuations. Due to arm asymmetries (mirror reflectivity, thermal effects), this phase noise couples into the main gravitational wave signal channel.   The Second Stage Frequency Stabilization (SSFS) loop uses reflected optical signals at POP, measured via photodiode B4, to compute correction signals for the laser.   This loop is influenced by time-varying transfer functions due to thermal drifts and asymmetry fluctuations. To model this, I introduced time-dependent gains:      \\(F_{PC}(t)\\): Power Coupling Gain   \\(F_{TC}(t)\\): Transfer Coupling Gain   \\(Y(f, t)\\): Injection Transfer Function   These parameters were calibrated using broadband noise injections (( N_{inj} )) and validated at times ( t_{inj} ). The model was used to explain differences between simulated and real data in DARM.     Conclusion   The feedback control systems in the Virgo interferometer are essential for keeping the detector in its optimal operating state. By modeling the noise propagation through these loops, it’s possible to:      Identify dominant noise sources   Design more effective control strategies   Predict the sensitivity of future detectors   Through this work, I demonstrated that:      The MICH loop is dominated by detection noise above 50Hz.   The SSFS loop requires time-varying modeling due to asymmetries.   Overall, this project not only validated important theoretical models, but also highlighted remaining challenges, such as unexplained bias and underestimation of some residual noise sources.      Future work should investigate whether these mismatches originate from modeling assumptions, unmodeled loop couplings, or unknown noise mechanisms.    ","url": "http://localhost:4000/Cperigois.github.io/gravitational-waves/Noisebudget"
  },{
    "title": "Recherche d'anomalie dans une série temporelle",
    "excerpt":"                                       Cette étude s’incrit dans une curiosité de ma part sur l’implémentation d’auteencodeur pour detecteur des anomalies electoniques en analyse du signal. Ce type d’étue est déja largement documenté sur paper with codes en voici quelques bonnes références :   . Pour moi c’est l’occasion de me formé a un nouveau domaine du deep learning les autoencodeurs                 appliqués aux séries temporelles que je connais déja bien.           L’objectif de cette étude est de tater les limite de l’utilisation d’autoencodeurs dans la recherche d’anomalies dans les signaux temporels. La question etant très vaste je me suis limité ici aux signaux sinusoidaux affecté par du bruit gaussien et un bruit de dérive typique de l’usure de composants electroniques. Pour ce billet je vous propose une petit résumé du fonctionnement des autoencodeurs, accompagné des parametres que j’ai choisit pour cette étude. Dans unes second partie de detaillerai la stratégie d’entrainement “étape par étape” qui permet de palier au probleme de périodicité de la fonction sinus. Enfin dans la derniere partie le model sera mis à l’épreuve avec une évaluation et des tests sur des données bruitées par un ratio de dérive.   Model IA   La base des autoencodeurs   “Ecrit une introduction basique aux autoencodeurs”   Mon model   “Voila mon model : class CNNEncoder(nn.Module):     def init(self, encoded_size):         super(CNNEncoder, self).init()         self.encoder = nn.Sequential(             nn.Conv1d(1, 16, kernel_size=5, stride=2, padding=2),  # [B, 16, 250]             nn.ReLU(),             nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),  # [B, 32, 125]             nn.ReLU(),             nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),  # [B, 64, ~63]             nn.ReLU(),             nn.Flatten(),             nn.Linear(64 * 63, encoded_size)         )   def forward(self, x):     return self.encoder(x)   class CNNDecoder(nn.Module):     def init(self, encoded_size):         super(CNNDecoder, self).init()         self.decoder_input = nn.Linear(encoded_size, 64 * 63)         self.decoder = nn.Sequential(             nn.Unflatten(1, (64, 63)),             nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=1),  # [B, 32, ~125]             nn.ReLU(),             nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),  # [B, 16, ~250]             nn.ReLU(),             nn.ConvTranspose1d(16, 1, kernel_size=5, stride=2, padding=2, output_padding=1),   # [B, 1, ~500]         )   def forward(self, x):     x = self.decoder_input(x)     x = self.decoder(x)     return x   class CNNAutoencoder(nn.Module):     def init(self, encoded_size):         super(CNNAutoencoder, self).init()         self.encoder = CNNEncoder(encoded_size)         self.decoder = CNNDecoder(encoded_size)         self.encoded_size = encoded_size   def forward(self, x):     latent = self.encoder(x)     reconstructed = self.decoder(latent)     return reconstructed ecrit un court paragraphe pour le décrire \"   Entrainement du model   Les données d’entrainement   L’entrainement est réalisé sur des sets de 12000 signaux de 500s, envoyés par patch de 32 au GPU.   La stratégie étape par étape   “L’entrainement du modele é été réalisé étape par étape, voici les différentes étapes et leurs caractéristique, il faudrait les repertorier dans un tableau. Etape 1 : La sunisoïde pure, sur 10 epochs, amplitude 4, phase 0, fréquence 0.05Hz, bruit Aucun Etape 2 : Légere variation de l’amplitude, sur 30 epochs, amplitude Uniform(3.5,4.5), phase 0, fréquence 0.05Hz, bruit Aucun Etape 3 : Et légère variation de la phase, sur 30 epochs, amplitude Uniform(3.5,4.5), phase Uniform(0,0.5), fréquence 0.05Hz, bruit Aucun Etape 4 : Et légère variation de la fréquence, sur 100 epochs, amplitude Uniform(3.5,4.5), phase Uniform(0,0.5), fréquence Uniform(0.025,0.075)Hz, bruit Aucun Etape 5 : Et ajout d’un faible bruit gaussien, sur 100 epochs, amplitude Uniform(3.5,4.5), phase Uniform(0,0.5), fréquence Uniform(0.025,0.075)Hz, bruit Loi normale(mu = 0, std = 0.05) Etape 6 : Augmantation de la variation des parametre de la sinusoïde, sur 250 epochs, amplitude Uniform(2,6), phase Uniform(0,2pi), fréquence Uniform(0.025,0.225)Hz, bruit Loi normale(mu = 0, std = 0.05) Etape 7 : Augmantation du bruit gaussien, sur 400 epochs, amplitude Uniform(2,6), phase Uniform(0,2pi), fréquence Uniform(0.025,0.225)Hz, bruit Loi normale(mu = 0, std = 0.25) Chaque étape de l’entrainement démarre avec un learning rate assez élevé de 1.e-4. Avec un scheduler, ReduceLROnPlateau, qui a chaque epoche verifie que l’entrainement d’ai pas atteint un seuil qui l’empeche de progresser, et si c’est le cas diminue le mearning rate. “   Le bruit résiduel   LEs signaux contenant du bruit gaussien il est impossible pour le modele de prédire parfaitement le signal d’entrée. Il y a donc dans cette étude un bruit résiduel. Si l’on reprend la formule de notre fonction de perte cf eqation précédente, on peut y decomposer le signal target comme la somme d’un signal informatif et d’un bruit aléatoire. En supposant que notre modele est parfait on a alors output = signal informatif. Il reste dons une erreur résiduelle :  mse = mean((output - (informativ_signal + random_noise)) ** 2) residual error = mean(random_noise ** 2) Dans le cas de notre étude le bruit résiduel sur la fonction de perte est de 0.0025 pour le bruit gaussien des étapes 4-6, puis de 0.0625 pour l’étape finale d’entrainement.   Ecrit une place pour que j’insert ici un graphique “sinusoïd_recognition.png” réprésentant l’évolution de la loss de validation pendant l’entrainement avec une légende que je pourrait modifier.   Evaluation et test sur des données bruitées   Evaluation du model   Le model est évalué sur 10000 nouveaux signaux générés comme a l’étape 7 de l’entrainement. On retrouve une valeur médiane de la perte de 0.078+0.053-0.012 ce qui est suffisament proche de l’erreur résiduelle pour notre cas d’études.   Les données bruitées  On mat notre model d’autoencodeur a l’épreuve d’un bruit de dérive “petite definition d’un bruit de dérive et formula mathématique ici”   Voici quelques exemple de signaux obtenus apres ajout du bruit de dérive pour quatra valeur différente “Ici j’insere une image avec 4 graphe représentant le signal avec sa composante dérive et sa reconstitution par le model”   Performance de détection d’anomalie   Comme on l’attendait, le model n’arrive pas a reproduire le signal d’entrée si il y a un bruit supplémentaire auquel il n’est pas entrainé. Sur le graphique suivant j’ai quantifié le score de perte de 1000 signaux pour 8 valeurs différentes de ratios de dérive. J’ai choisit de placer trois zones de confiance, lors du calcul de la perte.  OK : lorsque la perte est inférieur au 95 eme percentille de notre sample d’évaluation, le signal est avec confiance considéré comme normal. WARNING : lorsque la perte est comprise entre le 95eme percentille de du sample d’évaluation mais inférieure a la valaur maximale obtenue dans l’évaluation, le signal est considéré comme a risque, et la tendance doit etre surveillée. ANOMALY : au dela de la valeur maximale de perte obtenue dans le sample d’évaluation le signal est considéré comme anormale et des investigations plus poussées sont necessaires   Pour avoir une valeur comparable des résultats avec d’autres problemes j’ai évalué le Noise ratio defini comme Noise ratio = total_drift/residual error avec total_drift la dérive totale du signal sur sa durée. On constate que notre model detecte des anomalies de derive a partir de 1.6*bruit résiduel, ce qui est très performant. Ces performance pourraient etre encore augmenté en prenant des signaux plus long pour accentuer la dérive totale.   Conclusion   Les autoencodeurs sont efficace pour detecter des anomalies dans les signaux sinusoidaux bruités avec un ratio de dérive léger. Lorque ce ratio se rapprohe du bruit gaussien avec un Noise ratio &lt; 1.6, le bruit devient trop faible pour etre discerné du bruit blanc sur  la durée évaluée ici 500 secondes.   Ouverture : faire des test sur des signaux plus long permet de mettre en évidente des ratio de dérive plus faible.  J’ai affirmé dans mon étude que le probleme rencontré a l’entrainement direct avec les données complexee provenait e la caractéristique sinusoidale du signale, qui automatiquement donne un signal moyen null. Cette affiramtion mérite une investigation plus large (peut etre une future étude),il sera possible d’ajouter un offser, de travailler avec des fonctions de heavyside, ou encore des triangle. Ce cas reste un cas d’école, prototypé avec des données construitent, il faudrait mettre cette strategie a l’épreuve sur des données réelles.   ","url": "http://localhost:4000/Cperigois.github.io/IAlab/"
  },{
    "title": "Search for anomalies in sinusoidal signals",
    "excerpt":"                                       This study stems from a personal curiosity about the implementation of autoencoders for anomaly detection in signal processing. This type of approach is already widely documented on Papers With Code; here are a few relevant references. For me, it’s also an opportunity to get hands-on experience in a new area of deep learning: autoencoders applied to time series, a topic I already understand well from a signal analysis perspective.           The goal of this study is to test the limits of autoencoder-based anomaly detection in time-series signals. Given the breadth of the topic, I limited the study to sinusoidal signals affected by Gaussian noise and drift noise, the latter being typical of wear in electronic components.   This post consists of three sections:     A summary of how autoencoders work, with the parameters used in this study.   The “step-by-step” training strategy to address the periodic nature of the signals.   A model evaluation on noisy signals with a controlled drift ratio.     AI Model   Basics of Autoencoders   An autoencoder is a type of neural network designed to reconstruct its input after compressing it into a lower-dimensional latent space. It has two parts:      The encoder: reduces the input dimensionality to extract its essential features.   The decoder: reconstructs the input from the compressed representation.   In unsupervised learning, an autoencoder can learn dominant patterns in the data. Any significant reconstruction error then indicates an anomaly (i.e., data that deviates from the learned patterns).   The standard loss function is the Mean Squared Error:   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x}_i)^2\\]  where \\(x_i\\) is the original data and \\(\\hat{x}_i\\) is its reconstruction.     My Model   The model used in this study is a 1D convolutional autoencoder designed for fixed-length time signals (500 samples). It consists of two main blocks: an encoder and a decoder, built from convolutional layers suited to time-structured data.   Encoder   The encoder transforms the input signal into a low-dimensional latent representation. It includes:   Three successive 1D convolutional layers with:      Increasing numbers of channels (16, 32, then 64),   A kernel size of 5,   A stride of 2 to progressively reduce temporal resolution,   A ReLU activation after each convolution.   The output is then flattened into a linear vector and passed through a fully connected layer to produce a latent embedding of predefined size (encoded_size).   Decoder   The decoder reconstructs the original signal from the latent representation. It mirrors the encoder:      A first fully connected layer reshapes the latent vector into a tensor suitable for the transposed convolutional layers.   Three transposed convolution layers:            They progressively increase the temporal resolution,       Reduce the number of channels from 64 → 32 → 16 → 1 (the original shape),       Each step is followed by a ReLU activation, except the last.           This architecture allows the model to learn to extract key features from the input signal and reconstruct them as faithfully as possible. Latent compression forces the autoencoder to filter noise and retain only essential information.   This model encodes 500-sample input signals into a latent space sized at 30% of the input, and reconstructs them via transposed convolutions. The architecture is intentionally compact to facilitate progressive learning of simple patterns.     Model Training   Training Data   Training is conducted on batches of 12,000 signals, each 500 samples long, fed in mini-batches of 32 to the GPU. Each signal is a noisy sinusoid, randomly generated according to the parameters defined for each training phase.     Step-by-Step Training Strategy   Training progresses through increasingly complex signal generations. The summary table is as follows:                  Step       Epochs       Amplitude       Phase       Frequency (Hz)       Noise                       1       10       4       0       0.05       None                 2       30       \\(\\mathcal{U}(3.5, 4.5)\\)       0       0.05       None                 3       30       \\(\\mathcal{U}(3.5, 4.5)\\)       \\(\\mathcal{U}(0, 0.5)\\)       0.05       None                 4       100       \\(\\mathcal{U}(3.5, 4.5)\\)       \\(\\mathcal{U}(0, 0.5)\\)       \\(\\mathcal{U}(0.025, 0.075)\\)       None                 5       100       \\(\\mathcal{U}(3.5, 4.5)\\)       \\(\\mathcal{U}(0, 0.5)\\)       \\(\\mathcal{U}(0.025, 0.075)\\)       \\(\\mathcal{N}(0, 0.05)\\)                 6       250       \\(\\mathcal{U}(2, 6)\\)       \\(\\mathcal{U}(0, 2π)\\)       \\(\\mathcal{U}(0.025, 0.225)\\)       \\(\\mathcal{N}(0, 0.05)\\)                 7       400       \\(\\mathcal{U}(2, 6)\\)       \\(\\mathcal{U}(0, 2π)\\)       \\(\\mathcal{U}(0.025, 0.225)\\)       \\(\\mathcal{N}(0, 0.25)\\)           A ReduceLROnPlateau scheduler dynamically adjusts the learning rate from an initial value of ( 10^{-4} ), if validation loss plateaus.     Residual Noise   Due to the random nature of the added noise, perfect signal reconstruction is impossible. The final loss therefore stabilizes to a non-zero residual noise.   Assuming:   \\[\\text{signal}_{\\text{target}} = \\text{informative\\_signal} + \\text{random\\_noise}\\]  and perfect reconstruction of the informative part, the MSE becomes:   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n [(\\text{output} - \\text{target})^2] = \\frac{1}{n} \\sum_{i=1}^n [\\text{random\\_noise}^2]\\]  In this study:     Steps 4–6: residual noise ≈ 0.0025   Step 7: residual noise ≈ 0.0625      Blue : Validation loss evolution across the different training steps described on the previous table. Red : Learning rate evolution across the training. The grey zone represents the theoretical limits the model cannot overtake.     Evaluation and Testing on Noisy Signals   Model Evaluation   The model is evaluated on 10,000 new signals generated with the parameters of Step 7.  Observed median loss:   \\[\\text{MSE}_{\\text{test}} = 0.078^{+0.053}_{-0.012}\\]  This is compatible with residual noise, indicating good generalization capacity.     Drift Noise   A drift component was added to the signals:      Drift is a slow and continuous signal shift over time. It can be modeled with an affine function:    \\[\\text{drift}(t) = \\alpha \\cdot t\\]  where \\(\\alpha\\) is the drift coefficient.     Examples of drifted signals and their reconstruction for various values of \\(\\alpha\\).     Anomaly Detection   The model fails to reconstruct signals containing unseen drift, as expected. For each drift ratio, the loss is compared to evaluation percentiles:      OK: MSE &lt; 95ᵗʰ percentile   WARNING: 95ᵗʰ percentile &lt; MSE &lt; max   ANOMALY: MSE &gt; max (evaluation)   To quantify detection threshold, we define the noise ratio:   \\[\\text{Noise ratio} = \\frac{\\text{total\\_drift}}{\\text{residual\\_error}}\\]  The model detects anomalies as soon as the noise ratio reaches 1.6, which shows high sensitivity. This could be improved by increasing signal length.      Performance for drift signal reconstitution for various values of \\(\\alpha\\).     Conclusion   Autoencoders prove effective for detecting anomalies in sinusoidal signals contaminated by Gaussian noise and drift. The method is particularly sensitive to gradual deviations, as long as their amplitude exceeds the residual noise.   Next steps:     Use longer signals to detect subtler drift.   Explore other waveform types: triangular signals, Heaviside functions, etc.   Test the hypothesis that training difficulties stem from the zero-mean nature of sinusoids — e.g., by adding an offset.   This project is a proof-of-concept on simulated data. It would be relevant to test this approach on real-world data to assess its industrial potential.    ","url": "http://localhost:4000/Cperigois.github.io/BeyondTheLab/AILab/Search_anomaly"
  },{
    "title": "Recherche d'anomalie dans une série temporelle",
    "excerpt":"                                       Cette étude s’inscrit dans une curiosité personnelle concernant l’implémentation d’autoencodeurs pour la détection d’anomalies électroniques en analyse du signal. Ce type d’approche est déjà largement documenté sur Papers With Code ; voici quelques références pertinentes. Pour moi, c’est également l’occasion de me former à un nouveau domaine du deep learning : les autoencodeurs appliqués aux séries temporelles, un sujet que je maîtrise déjà bien du point de vue signal.           L’objectif de cette étude est de tester les limites de l’utilisation d’autoencodeurs dans la recherche d’anomalies dans les signaux temporels. Le sujet étant très vaste, je me suis limité ici à l’étude de signaux sinusoïdaux affectés par un bruit gaussien et un bruit de dérive, ce dernier étant typique de l’usure de composants électroniques.   Ce billet se compose de trois parties :     Un résumé du fonctionnement des autoencodeurs, avec les paramètres choisis pour cette étude.   La stratégie d’entraînement « étape par étape » pour contourner les difficultés liées à la périodicité des signaux.   Une évaluation du modèle sur des données bruitées contenant un ratio de dérive contrôlé.     Modèle IA   La base des autoencodeurs   Un autoencodeur est un type de réseau de neurones dont l’objectif est de reconstruire l’entrée après l’avoir compressée dans un espace latent de plus faible dimension. Il se compose de deux parties :      L’encodeur : réduit la dimensionnalité de l’entrée pour extraire ses caractéristiques essentielles.   Le décodeur : reconstruit l’entrée à partir de la représentation compressée.   En apprentissage non supervisé, un autoencodeur peut ainsi apprendre les motifs dominants des données. Toute erreur de reconstruction importante indique alors une anomalie (c’est-à-dire une donnée ne correspondant pas au motif appris).   La fonction de perte classique est l’erreur quadratique moyenne :   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x}_i)^2\\]  où ( x_i ) est la donnée originale et ( \\hat{x}_i ) sa reconstruction.     Mon modèle   Le modèle utilisé dans cette étude est un autoencodeur convolutionnel 1D conçu pour traiter des signaux temporels de longueur fixe (500 échantillons). Il se compose de deux blocs principaux : un encodeur et un décodeur, construits à partir de couches convolutives, adaptées aux données structurées dans le temps.   Encodeur   L’encodeur transforme le signal d’entrée en une représentation latente de dimension réduite (embedding). Il est constitué de :   Trois couches de convolution 1D successives avec :           Un nombre croissant de canaux (16, puis 32, puis 64),            Un noyau de convolution de taille 5,            Un stride de 2 pour réduire progressivement la résolution temporelle,            Une activation ReLU après chaque convolution.       À la fin de ces couches, la sortie est applatie (flatten) pour former un vecteur linéaire, qui est ensuite projeté via une couche linéaire (fully connected) vers l’espace latent de taille définie (encoded_size).   Décodeur   Le décodeur reconstruit le signal original à partir de la représentation latente. Il réalise l’opération inverse de l’encodeur, avec :   Une première couche linéaire pour transformer le vecteur latent en un tenseur compatible avec la structure attendue par les couches de déconvolution.   Trois couches de convolution transposée (aussi appelées “déconvolutions”) :   Elles augmentent progressivement la taille du signal dans le temps,   Réduisent le nombre de canaux de 64 à 32, puis 16, puis 1 (la forme du signal d’origine),   Chaque étape est suivie d’une activation ReLU, sauf la dernière.   Ce design permet au modèle d’apprendre à extraire les motifs caractéristiques du signal d’entrée, puis à les reconstruire aussi fidèlement que possible. La compression dans l’espace latent force l’autoencodeur à filtrer le bruit et à ne conserver que les informations essentielles.   Ce modèle encode des signaux d’entrée de taille 500 vers un espace latent de dimension fixée à 30% de la taille d’entrée dans notre cas, avant de les reconstituer via des couches convolutionnelles transposées. L’architecture est volontairement compacte pour faciliter l’apprentissage progressif de motifs simples.     Entraînement du modèle   Les données d’entraînement   L’entraînement est réalisé sur des lots de 12000 signaux de 500 secondes, envoyés par batchs de 32 au GPU. Chaque signal est une sinusoïde bruitée, générée aléatoirement selon les paramètres de l’étape d’apprentissage.     Stratégie d’apprentissage par étapes   L’entraînement suit une progression par complexification croissante des signaux. Voici un résumé en tableau :                  Étape       Époques       Amplitude       Phase       Fréquence       Bruit                       1       10       4       0       0.05 Hz       Aucun                 2       30       Uniform(3.5, 4.5)       0       0.05 Hz       Aucun                 3       30       Uniform(3.5, 4.5)       Uniform(0, 0.5)       0.05 Hz       Aucun                 4       100       Uniform(3.5, 4.5)       Uniform(0, 0.5)       Uniform(0.025, 0.075) Hz       Aucun                 5       100       Uniform(3.5, 4.5)       Uniform(0, 0.5)       Uniform(0.025, 0.075) Hz       Bruit gaussien ( \\mathcal{N}(0, 0.05) )                 6       250       Uniform(2, 6)       Uniform(0, 2π)       Uniform(0.025, 0.225) Hz       Bruit gaussien ( \\mathcal{N}(0, 0.05) )                 7       400       Uniform(2, 6)       Uniform(0, 2π)       Uniform(0.025, 0.225) Hz       Bruit gaussien ( \\mathcal{N}(0, 0.25) )           Un scheduler de type ReduceLROnPlateau est utilisé pour ajuster dynamiquement le learning rate à partir d’une valeur initiale de ( 10^{-4} ), en cas de stagnation des performances.     Le bruit résiduel   Le modèle ne peut reconstruire parfaitement le signal d’entrée, à cause de la composante aléatoire du bruit. La perte finale atteint donc une valeur non nulle, appelée bruit résiduel.   Considérons :   \\[\\text{signal}_{\\text{target}} = \\text{signal}_{\\text{informatif}} + \\text{bruit}_{\\text{aléatoire}}\\]  et supposons une reconstruction parfaite du signal informatif. Alors, l’erreur moyenne devient :   \\[\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n [(\\text{output} - \\text{signal}_{\\text{target}})^2] = \\frac{1}{n} \\sum_{i=1}^n [\\text{bruit}_{\\text{aléatoire}}^2]\\]  Dans notre étude :     Étapes 4–6 : bruit résiduel ≈ 0.0025   Étape 7 : bruit résiduel ≈ 0.0625      Évolution de la perte de validation durant l’entraînement (légende à adapter).     Évaluation et tests sur signaux bruités   Évaluation du modèle   Le modèle est évalué sur 10 000 nouveaux signaux générés avec les paramètres de l’étape 7.  La perte médiane observée est :   \\[\\text{MSE}_{\\text{test}} = 0.078^{+0.053}_{-0.012}\\]  Ce qui reste compatible avec le bruit résiduel, indiquant un bon pouvoir de généralisation.     Les données bruitées   Un bruit de dérive a été ajouté aux signaux :      Une dérive est une modification lente et continue du signal dans le temps. Elle peut être modélisée par une fonction affine :    \\[\\text{drift}(t) = \\alpha \\cdot t\\]  où ( \\alpha ) est le coefficient de dérive.      Exemples de signaux avec dérive pour quatre valeurs de ( \\alpha ), comparés à leur reconstruction.     Détection d’anomalie   Le modèle échoue à reconstruire correctement les signaux contenant une dérive non apprise, comme attendu. Pour chaque ratio de dérive, la perte est comparée aux percentiles de l’évaluation :      OK : MSE &lt; 95ᵉ percentile   WARNING : 95ᵉ percentile &lt; MSE &lt; max   ANOMALY : MSE &gt; max (évaluation)   Pour quantifier le seuil de détection, on définit le noise ratio :   \\[\\text{Noise ratio} = \\frac{\\text{total\\_drift}}{\\text{erreur résiduelle}}\\]  Le modèle détecte des anomalies dès que le noise ratio atteint 1.6, ce qui indique une très bonne sensibilité. Cette performance pourrait être améliorée en augmentant la durée des signaux.      Exemples de signaux avec dérive pour quatre valeurs de ( \\alpha ), comparés à leur reconstruction.     Conclusion   Les autoencodeurs se montrent efficaces pour détecter des anomalies dans des signaux sinusoïdaux contaminés par du bruit gaussien et de la dérive. La méthode est particulièrement sensible aux déviations progressives, tant que leur amplitude dépasse le bruit résiduel.   Perspectives :     Travailler sur des signaux plus longs pour détecter des dérives plus faibles.   Explorer des alternatives aux sinusoïdes : signaux triangulaires, fonctions de Heaviside, etc.   Vérifier l’hypothèse selon laquelle les difficultés d’entraînement proviennent de la moyenne nulle des sinusoïdes — en ajoutant par exemple un offset.   Ce travail constitue un prototype sur données simulées. Il serait pertinent de valider cette approche sur des données réelles pour en mesurer le potentiel industriel.    ```  ","url": "http://localhost:4000/Cperigois.github.io/IAlab/"
  },{
    "title": "All projects",
    "excerpt":"A list of all projects presented on this website. Projects tagged as “independent” are self-initiated,  driven by curiosity, and developed as part of my self-learning journey.   My projects                                  Noise modelisation and propagation in gravitational wave detector                                       Statistics            Regression            Frequency- Spectrum            Time-Series            Matlab            Simulink            Modeling                                          Study and modelisation of noise propagation through control loops in the Virgo detector                                                                Search for anomalies in sinusoidal signals                                       Autoencodeur            CNN            Simulation            Time-Series            Python            PyTorch            Modeling                                          Proof of concept (PoC) demonstrating the efficiency of autoencoders for anomaly detection in time series signals.                                                                                                                                                             ","url": "http://localhost:4000/Cperigois.github.io/all-projects/"
  },{
    "title": {"en":"Black holes & gravitational waves","fr":"Trous noirs & ondes gravitationnelles"},
    "excerpt":"Pour introduire mes travaux de recherche je propose de les integrer dans trois présentations vulgarisées plus larges qui permettront d’evaluer le contexte général des projets que j’ai eu le plaisir de mener pendant ma carriere. Vous trouverez donc trois pages de recherche :      Les ondes gravitationnelles et leur détections   Aux origines des trous noirs   ??   Sur cette première page j’introduis les ondes gravitationnelles dns le but de présenter mes travaux de recherches et les projets  auxquels j’ai eu le plaisir de participer.   Une introduction historique  Les ondes gravitationnelles sont des perturbations de l’espace temps qui se propagent à la vitesse de la lumière. Elles  sont une conséquence directe de la theorie de la relativité générales d’Albert Einstein et des ses collaborateurs.ices  soumise en 1915, sous la forme de deux papiers principaux : “On the general theory of relativity”  et “The Field Equations of Gravitation”. La première publication  formalisant les ondes gravitationnelles “Über Gravitationswellen”  quant a elle arrive 3 ans plus tard.                                         Abstract de l’article original sur les ondes gravitationnelles en 1918 par Albert Einstein           En 1974, Rusell Hulse et Joseph Taylor, découvrent le pulsar binaire PSR B1913+16, c’est a dire un système de deux objet  en orbite dont l’un est un pulsar, qui a intervalle régulier émet un pulse de lumière. En 1979 après quatre ans de  surveillance des caratérisiques du systèmes, Hulse et Taylor mettent en évidence que l’évolution de l’orbite du systeme  reproduit parfaitement les prédictions de la théorie de la gravitation pour un système binaire (Lien vers la publication. Cette découverte leur apportera un prix nobel en 1993. La publication la plus récente a ce jour (2016) qui répertorie ces  mesures est en libre acces à ce lien : Denière publication de 2016                                         Graphique représentant la décroissance de la période du pulsar binaire. Les points rouges représentent les prises de données, avec leur incertitudes (quasi invisible car faibles). La courbe représente la prédiction de la théorie de la gravitation           C’est l’année du centième anniversaire de la théorie de la relativité générale, en 2015, que la première mesure directe  d’onde gravitationnelles à eu lieu. Les détecteurs américains LIGO détecter alors la fusion de deux trous noirs de  respectivement 29 et 36 masses solaires (Lien vers l’article)   A partir de 2017,lors de la seconde phase de prise de données (O2), le détecteur européen rejoint le reseau américain, et permet la  traiangulation du signal pour reconstruire la provenance des sources d’ondes gravitaionnelles. Le 17 aout 2017, le nouveau  réseau observe une fusion de deux étoiles à neutrons conjointement avec les ondes electromagnétiques  (Lien vers la publication,  Lien vers l’article de la détection multimessagère. C’est la naissance  le l’astronomie multimessagère c’est à dire la construction d’une communauté pluridisciplinaire, qui permet d’observer les  phénomènes astrophysiques avec différents messagers : les ondes electromagnétiques, les rayons cosmiques, les neutrinos et  les ondes gravitationnelles. C’est également en 2017 que les travaux de la premières détections, GW150914, sont récompensés d’un prix nobel.   Au jour de l’écriture de cette page (2025) le reseaux de détecteurs d’ondes gravitationnel compte 4 instruments, deux  sur le sol americain, un en italie, et un au Japon. La communauté dénombre desormais plus d’une centaine d’observation de  fusions de trous noirs, offrant à la communauté astrophysiques des données précieuses sur l’histoire et l’évolution de l’univers.   Detecter les ondes gravitationnelles      Présentation à la conférence X  Paris, Mars 2022     ","url": "http://localhost:4000/Cperigois.github.io/fr/gravitational-waves/"
  },{
    "title": "Black Holes & Gravitational Waves",
    "excerpt":"To introduce my research work, I present it through three broader, popular-science-oriented sections. These allow a general contextualization of the scientific projects I have had the pleasure of conducting throughout my career. You will find three main topics:      Gravitational waves and their detection   The origin of black holes   (Upcoming content)   This first page is dedicated to gravitational waves, offering an overview of the field and introducing my related research projects.     A Brief Historical Introduction   Gravitational waves are ripples in spacetime that propagate at the speed of light. They are a direct consequence of Albert Einstein’s theory of general relativity, formulated in 1915 in two foundational papers:     “On the General Theory of Relativity”   “The Field Equations of Gravitation”   The first publication explicitly describing gravitational waves—“Über Gravitationswellen” (1918)—appeared three years later.     Abstract of the original 1918 article by Albert Einstein   In 1974, Russell Hulse and Joseph Taylor discovered the binary pulsar PSR B1913+16—two orbiting stellar remnants, one of which is a pulsar emitting regular light pulses. In 1979, after four years of observation, they demonstrated that the orbital decay of the system matched the predictions of general relativity, thus confirming the existence of gravitational waves indirectly (1979 publication). They were awarded the Nobel Prize in 1993.  A 2016 update to this work is available here.     Orbital decay of the binary pulsar system. Red points are measurements, closely matching the theoretical curve predicted by general relativity.   The first direct detection of gravitational waves occurred in 2015—exactly 100 years after general relativity. Using the LIGO detectors, scientists observed the merger of two black holes (29 and 36 solar masses).  (Original paper)   In 2017, the Virgo detector in Europe joined the LIGO network, enabling triangulation of gravitational wave sources. On August 17, 2017, the network observed the merger of two neutron stars, simultaneously with electromagnetic signals, marking the beginning of multi-messenger astronomy.     Detection paper   Multi-messenger analysis   The foundational detection (GW150914) was recognized with the 2017 Nobel Prize in Physics.   As of 2025, the global network includes four detectors: two in the U.S. (LIGO), one in Italy (Virgo), and one in Japan (KAGRA). Together, they have recorded over 100 black hole mergers, enriching our understanding of the universe’s structure and evolution.     Ground-Based Gravitational Wave Detectors   At the time of writing, four operational ground-based interferometers exist:      LIGO Livingston and LIGO Hanford (USA) – each with 4 km-long arms   Virgo (Italy) – 3 km-long arms   KAGRA (Japan) – a 3 km-long underground interferometer      How These Detectors Work   Gravitational waves slightly stretch and squeeze spacetime. On Earth, this is measured using giant Michelson interferometers with kilometer-scale arms. The principle involves detecting phase shifts in laser beams bouncing between mirrors at the ends of the arms. A passing wave causes a tiny differential change in path length, producing interference patterns that can be measured.                 Click the image to watch the explicative video on YouTube    Operating Point of an Interferometer   Interferometers are tuned to operate on a dark fringe, meaning destructive interference cancels the signal at the output. This requires real-time feedback control of mirror positions to maintain stability.  In this section, I present my research on modeling and simulating these feedback control loops to keep the interferometer at its operating point.     The Future of Ground-Based Detectors   Two major next-generation projects are currently underway:   🇺🇸 Cosmic Explorer (USA)   The Cosmic Explorer collaboration proposes building two new interferometers with arm lengths of 20 to 40 km, dramatically improving sensitivity. This would allow detection of nearly all compact binary mergers in the observable universe.  🔗 Cosmic Explorer website  🖼️ Insert CE image here   🇪🇺 Einstein Telescope (Europe)   Europe is preparing the Einstein Telescope (ET), possibly built at one or two sites. Multiple design and location options are still under study.  My work explores the scientific impact of design choices on compact binary detection rates and broader scientific potential.     ET official site   Design impact study (2023)   ET Blue Book (2025)     Pulsar Timing Arrays   Another method of detecting gravitational waves is by monitoring millisecond pulsars. These highly regular signals can exhibit arrival time delays if spacetime is distorted by a passing gravitational wave. By comparing dozens of pulsars, it’s possible to reveal such distortions.   In June 2023, the NANOGrav collaboration announced evidence of a gravitational wave background, possibly generated by supermassive black hole binaries or a population of such systems.  🔗 NANOGrav discovery paper     Space-Based Interferometers   LISA (Laser Interferometer Space Antenna) is an ambitious project led by ESA in collaboration with NASA.  It will consist of three spacecraft placed in a triangle formation orbiting around the Lagrange Point L2, with arms stretching 2.5 million kilometers. This enables exploration of low-frequency gravitational waves, inaccessible from the ground.     Would you like this content broken into multiple subpages or left as one long article? I can also generate a summary or add visual elements if needed.   ","url": "http://localhost:4000/Cperigois.github.io/gravitational-waves/"
  },{
    "title": "Black holes & gravitational waves",
    "excerpt":"To introduce my research work, I propose to integrate it into three broader popular presentations that will allow for an  evaluation of the general context of the projects I have had the pleasure of conducting throughout my career. You will find three research pages:      Gravitational waves and their detection   The origins of black holes   ??   On this first page, I introduce gravitational waves with the aim of presenting my research work and the projects I have  had the pleasure of participating in.   An historical introduction   Gravitational waves are disturbances in spacetime that propagate at the speed of light. They are a direct consequence of  Albert Einstein’s general theory of relativity and his collaborators, submitted in 1915, in the form of two main papers: “On the general theory of relativity”  and “The Field Equations of Gravitation”. The first publication  formalizing gravitational waves “Über Gravitationswellen” , came three years later.                                         *Abstract of the original article on gravitational waves in 1918 by Albert Einstein *           In 1974, Russell Hulse and Joseph Taylor discovered the binary pulsar PSR B1913+16, which is a system of two objects in orbit,  one of which is a pulsar that emits a pulse of light at regular intervals. In 1979, after four years of monitoring the  characteristics of the system, Hulse and Taylor demonstrated that the evolution of the system’s orbit perfectly reproduces  the predictions of gravitational theory for a binary system (Link to the publication. This discovery earned them a Nobel Prize in 1993. The most recent publication to date (2016) that catalogs these measurements  is freely accessible at this link:  : Last publication from 2016                                         Graph showing the decay of the binary pulsar’s period. The red points represent data points, with their uncertainties (almost invisible due to being small). The curve represents the prediction of gravitational theory           The first direct detection of gravitational waves occurred in 2015—the 100th anniversary of general relativity. Using the LIGO detectors, scientists observed the collision of two black holes, one with 29 times the mass of the Sun and the other with 36 times the Sun’s mass.(Lien vers l’article)   Starting in 2017, during the second data-taking phase (O2), the European detector joined the American network, enabling signal triangulation to locate the sources of gravitational waves. On August 17, 2017, the new network observed a merger of two neutron stars  simultaneously with electromagnetic waves.(Lien vers la publication,  Lien vers l’article de la détection multimessagère. This marked the birth of multimessenger astronomy—the creation of an interdisciplinary community enabling the observation of astrophysical phenomena through different cosmic messengers: electromagnetic waves, cosmic rays, neutrinos, and gravitational waves.   Also in 2017, the groundbreaking work behind the first detection, GW150914, was honored with a Nobel Prize.   As of this writing (2025), the gravitational-wave detector network comprises four instruments: two in the U.S., one in Italy, and one in Japan. The scientific community has now recorded over a hundred black hole mergers, providing astrophysicists with invaluable data on the history and evolution of the universe.   Les détecteurs terrestres d’onde gravitationnels   insertion worldmap   A l’ecriture de ce site il existe 4 détecteurs d’ondes gravitationnelles sur terre deux aux états unis avec des bras de 4 km de long chacun, Virgo en Italie avec des bras de 3km et KAGRA au Japon, interférometre souterrain de 3 km de long.   Fonctionnement des détecteurs.   La détection d’ondes gravitationnelles est réalisée par la mise en évidence d’une dilatation ou rétraction de l’espace-temps. Sur terre, ce phénomène est mesurée avec des interférometre de Michelson géants dont les bras font plusieurs kilometres de long. L’objectif de ces dispositifs est de mesurer une interférence entre les faisceaux lumineux de chacun des deux bras de l’interférometre. Un éventuel dephasage temoignera d’une variation dans la distance parcourue par le faisceau lumineux. En mesurant ce déphasage en temps réel il est alors possible de reconstruire le laçage d’une onde gravitationnelle.   Video explicative du fonctionnement : https://www.youtube.com/watch?v=UA1qG7Fjc2A   Point de fonctionnement d’un interférometre. En pratique l’interférometre est calibré sur une frange noire, de sorte que la somme des signaux reconstruit avant la mesure par la photodiode de sortie soit quasi-nulle. Cette contrainte implique un controle précis et automatique de la position des miroirs. Dans ce billet je vous présente mes travaux sur la modélisation des boucles de rétroaction qui permettent de maintenir l’interféromètre a son point de fonctionnement.   Les futurs détecteurs terrestres   Aujourd’hui il y a deux projets majeurs de construction de nouveaux instruments sur terre. Une collaboration etat unienne Cosmic explorer, envisage de construire deux nouveaux interférometres, de 20 et/ou 40 km de longueurs de bras. Cet allongement des bras permettra d’avoir une meilleure sensibilité et d’observer la quasi totalité des fusion d’objets compact de l’univers. Lien vers la page de CE : https://cosmicexplorer.org/index.html  Nom de la photo pour CE donuts truc   La communauté européene prépare également la construction d’un (ou deux) instruments de mesures. Plusieurs design sont encore envisagé et plusieurs localisation. Dans ce billet je reprend l’etude d’impact de design sur la detection de systeme binaire d’objetc compacts et les grand enjeux scientifiques de cettre nouvelle génération de détecteurs. lien vers le site ET : https://www.einsteintelescope-emr.eu/fr/   Etude d’impact des design : https://ui.adsabs.harvard.edu/abs/2023JCAP…07..068B/abstract Blue book : https://ui.adsabs.harvard.edu/abs/2025arXiv250312263A/abstract   Les réseaux de pulsars   Il existe une seconde méthode pour observer les ondes gravitationnelles qui consiste a surveiller des pulsar milliseconde avec les télescope terrestre. Ces pulsars envoie des pulses lumineux parfaitement régulier, dont l’éventuel retard d’arrivée témoigne du passage d’une onde gravitationnelle. En comparant les temps d’arrivée des pulses de plusieurs dizaines de pulsars, il est possible de mettre en évidence le passage d’une onde gravitationnelle. En juin 2023, la collaboration Nanograv annonce l’observation d’un fond gravitationnel dont l’origine fait débat. Les hypothèses privilégiés proposent des ondes émisent par une binaire de trous noirs supermassif, ou une population de ces binaires, dont les ondes gravitationnelles se superposeraient. Lien vers la publication de NAnograv : https://ui.adsabs.harvard.edu/abs/2023ApJ…951L…9A/abstract   Les interféromètres spatiaux   LISA est le projet Européen de l’ESA (European space agency) en collaboration avec la NASA. L’instrument sera composé de trois modules disposés en triangle autour d’un point de lagrange L2? Ces instruments seront éloignés de 2.5 millions de kilomètres, et permettent d’explorer une nouvelle plage de fréquence pour les ondes gravitationnelles.   ","url": "http://localhost:4000/Cperigois.github.io/gravitational-waves/old"
  },{
    "title": {"en":"Data science","fr":"Data science"},
    "excerpt":"Projets en cours                                                                                                                       Princess                                                    Outil open source pour prédire les ondes gravitationnelles à partir de modèles astrophysiques.                                                   En savoir plus                                                                                                                                                    Détection de plantations de palmiers                                                    Algorithme de deep learning pour identifier les plantations d’huile de palme sur images satellites. En développement.                                                   En savoir plus                                                                                                                                                    Cours d'initiation à Python                                                    Des bases du langage Python jusqu’à l’architecture logicielle avancée                                                   En savoir plus                                       ","url": "http://localhost:4000/Cperigois.github.io/"
  },{
    "title": "Data science",
    "excerpt":"Current projects                                                                                                                       Princess                                                    Open source tool to predict gravitational waves from astrophysical models.                                                   Learn more                                                                                                                                                    Palm oil farms searches                                                    Deep learning algorithm to detect palm oil farms from satellite images. Under developement                                                   Learn more                                                                                                                                                    Search for anomalies in Time Series                                                    Developpement of an autoencoder to detect anomalies such as drift in sinusoïdal signals.                                                   Learn more                                        ","url": "http://localhost:4000/Cperigois.github.io/"
  }]
